{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curves and AUC are great ways to evaluate how well statistical and machine learning models predict outcomes. Specifically, it gives us a good summary of the performance of our model based on \n",
    "\n",
    "Here is an example of an ROC curve with confidence intervals. The code used to generate this can be seen below. \n",
    "![](https://dl.dropboxusercontent.com/u/97258109/Screens/S3728.png.jpg)\n",
    "\n",
    "AUC is the area under the ROC curve and is a very popular way to measure the performance of classification problems. The ROC box is a unit square (area 1) and so a value of 1.0 is perfect because it represents 100% of the area of the square. A value of 0.5 is a completely useless model. If we have an ROC partially below our diagonal, we can just bet against our model to get a prediction better than random chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using it in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In R, once we make a prediction[1] we can plot the ROC seen above with the following code:\n",
    "\n",
    "```R\n",
    "library(pROC)\n",
    "p = predict(model, newdata=subset(test,select=c(2,3,4,5,6,7,8)), type=\"response\")\n",
    "roc(test$Survived, p)\n",
    "\n",
    "plt = plot.roc(test$Survived, p, \n",
    "               main=\"ROC w/ Confidence Intervals\", percent=TRUE, \n",
    "               ci=TRUE,  print.auc=TRUE) \n",
    "\n",
    "ciobj = ci.se(plt, specificities=seq(0, 100, 5))\n",
    "plot(ciobj, type=\"shape\", col=\"#1c61b6AA\") # Cool shade of blue\n",
    "plot(ci(plt, of=\"thresholds\", thresholds=\"best\"))\n",
    "\n",
    "\n",
    "```\n",
    "Where the `Survived` is the value that we are trying to predict and `p` is the model that fit using the `predict` function.\n",
    "\n",
    "I prefer `pROC` library over the `ROCR` library because it makes it easy to plot confidence intervals and \n",
    "\n",
    "Note that different packages use different terms for the x and y axes. Other people and libraries prefer the terms false positive rate (1 - specificity) and true positive rate (sensitivity). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using it in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "def generate_roc(X, y, model):\n",
    "    \"\"\"\n",
    "    Takes a vector of features, X, a vector targets, y and draws an\n",
    "    ROC along with AUC scores.\n",
    "\n",
    "    :param X: numpy array (k x N) of features.\n",
    "    :param y: numpy array (N x 1) of labels.\n",
    "    :param model: an sklearn ALREADY FITTED classifer\n",
    "    :return: draws a plot of ROC\n",
    "\n",
    "    Notes\n",
    "    ---\n",
    "    - Meant to be run in pylab inline mode.\n",
    "    - Only works with classification models.\n",
    "\n",
    "    \"\"\"\n",
    "    y_pred = model.predict_proba(X)[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_pred)\n",
    "    auc = roc_auc_score(y, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(fpr, tpr, label='ROC Curve. AUC = {0:.2f}'.format(auc))\n",
    "\n",
    "    # Plot our 45 degree line and make it a dashed dot\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "\n",
    "    # Axis work\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This renders a plot that looks like this:\n",
    "\n",
    "![](https://dl.dropboxusercontent.com/u/97258109/Screens/S3729.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with the AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is a problematic metric in many ways. If the ROC curves of different classifiers cross, it is possible hat one curve has a larger AUC and the other one may show superior performance over almost the entire range of values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://web.cs.iastate.edu/~cs573x/Notes/hand-article.pdf\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
